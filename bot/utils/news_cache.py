import os
import json
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from bot.config import config

logger = logging.getLogger(__name__)

class NewsCache:
    def __init__(self):
        self.archive_path = Path(config.NEWS_ARCHIVE_PATH)
        self.max_age_hours = config.NEWS_CACHE_HOURS
        self.lock = asyncio.Lock()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∞—Ä—Ö–∏–≤–∞ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.archive_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"üìÅ –ê—Ä—Ö–∏–≤ –Ω–æ–≤–æ—Å—Ç–µ–π: {self.archive_path}")
    
    def _get_category_file(self, category: str) -> Path:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        return self.archive_path / f"{category}.json"
    
    def _get_daily_archive_file(self, date: datetime) -> Path:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ –¥–Ω–µ–≤–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É"""
        date_str = date.strftime("%Y-%m-%d")
        return self.archive_path / "daily" / f"{date_str}.json"
    
    async def save_news(self, category: str, news_list: List[Dict]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        async with self.lock:
            try:
                category_file = self._get_category_file(category)
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                cache_data = {
                    'category': category,
                    'updated_at': datetime.now().isoformat(),
                    'news_count': len(news_list),
                    'news': news_list
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                with open(category_file, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
                # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –¥–Ω–µ–≤–Ω–æ–π –∞—Ä—Ö–∏–≤
                await self._save_to_daily_archive(news_list)
                
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π {category}: {e}")
                return False
    
    async def _save_to_daily_archive(self, news_list: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ –¥–Ω–µ–≤–Ω–æ–π –∞—Ä—Ö–∏–≤"""
        try:
            today = datetime.now()
            daily_file = self._get_daily_archive_file(today)
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            daily_file.parent.mkdir(parents=True, exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∞—Ä—Ö–∏–≤ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            daily_data = []
            if daily_file.exists():
                with open(daily_file, 'r', encoding='utf-8') as f:
                    daily_data = json.load(f)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
            existing_ids = {item.get('id') for item in daily_data}
            new_items = [item for item in news_list if item.get('id') not in existing_ids]
            
            if new_items:
                daily_data.extend(new_items)
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                daily_data.sort(key=lambda x: x.get('published_timestamp', 0), reverse=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∞—Ä—Ö–∏–≤
                with open(daily_file, 'w', encoding='utf-8') as f:
                    json.dump(daily_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"üìÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –¥–Ω–µ–≤–Ω–æ–π –∞—Ä—Ö–∏–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –¥–Ω–µ–≤–Ω–æ–π –∞—Ä—Ö–∏–≤: {e}")
    
    async def load_news(self, category: str) -> Optional[List[Dict]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            category_file = self._get_category_file(category)
            
            if not category_file.exists():
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            
            with open(category_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∫–µ—à–∞
            updated_at = datetime.fromisoformat(cache_data.get('updated_at', ''))
            if datetime.now() - updated_at > timedelta(hours=self.max_age_hours):
                logger.warning(f"‚ö†Ô∏è –ö–µ—à –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category} —É—Å—Ç–∞—Ä–µ–ª")
                return None
            
            news_list = cache_data.get('news', [])
            logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}")
            return news_list
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π {category}: {e}")
            return None
    
    async def get_latest_news(self, limit: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        try:
            all_news = []
            
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            for category_file in self.archive_path.glob("*.json"):
                if category_file.name == "cache_stats.json":
                    continue
                
                try:
                    with open(category_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    news_list = cache_data.get('news', [])
                    all_news.extend(news_list)
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {category_file}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ
            all_news.sort(key=lambda x: x.get('published_timestamp', 0), reverse=True)
            latest_news = all_news[:limit]
            
            logger.info(f"üì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(latest_news)} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return latest_news
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
    
    async def cleanup_old_news(self):
        """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
        async with self.lock:
            try:
                logger.info("üßπ –ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
                
                cutoff_time = datetime.now() - timedelta(hours=self.max_age_hours)
                cleaned_files = 0
                
                # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                for category_file in self.archive_path.glob("*.json"):
                    if category_file.name == "cache_stats.json":
                        continue
                    
                    try:
                        with open(category_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        updated_at = datetime.fromisoformat(cache_data.get('updated_at', ''))
                        
                        if updated_at < cutoff_time:
                            category_file.unlink()
                            cleaned_files += 1
                            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ñ–∞–π–ª: {category_file.name}")
                    
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {category_file}: {e}")
                
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–Ω–µ–≤–Ω—ã–µ –∞—Ä—Ö–∏–≤—ã
                daily_dir = self.archive_path / "daily"
                if daily_dir.exists():
                    for daily_file in daily_dir.glob("*.json"):
                        try:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                            date_str = daily_file.stem
                            file_date = datetime.strptime(date_str, "%Y-%m-%d")
                            
                            if file_date < cutoff_time:
                                daily_file.unlink()
                                cleaned_files += 1
                                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –¥–Ω–µ–≤–Ω–æ–π –∞—Ä—Ö–∏–≤: {daily_file.name}")
                        
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–Ω–µ–≤–Ω–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ {daily_file}: {e}")
                
                logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —É–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {cleaned_files}")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
    
    async def get_cache_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞"""
        try:
            stats = {
                'archive_location': str(self.archive_path),
                'total_news': 0,
                'categories': {},
                'daily_archives': 0,
                'cache_size_mb': 0
            }
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            for category_file in self.archive_path.glob("*.json"):
                if category_file.name == "cache_stats.json":
                    continue
                
                try:
                    with open(category_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    category = cache_data.get('category', category_file.stem)
                    news_count = cache_data.get('news_count', 0)
                    updated_at = cache_data.get('updated_at', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    
                    stats['categories'][category] = {
                        'count': news_count,
                        'last_updated': updated_at
                    }
                    stats['total_news'] += news_count
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ {category_file}: {e}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–Ω–µ–≤–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤
            daily_dir = self.archive_path / "daily"
            if daily_dir.exists():
                stats['daily_archives'] = len(list(daily_dir.glob("*.json")))
            
            # –†–∞–∑–º–µ—Ä –∫–µ—à–∞
            total_size = sum(f.stat().st_size for f in self.archive_path.rglob("*.json"))
            stats['cache_size_mb'] = round(total_size / (1024 * 1024), 2)
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞: {e}")
            return {'error': str(e)}

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–µ—à–∞
news_cache = NewsCache()
