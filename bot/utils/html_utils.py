import re
import html
from typing import Dict, Optional
from urllib.parse import urlparse

def clean_html_entities(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç HTML entities –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""
    
    # –°–Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π html.unescape
    text = html.unescape(text)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–º–µ–Ω—ã –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    replacements = {
        '&laquo;': '¬´',
        '&raquo;': '¬ª',
        '&ldquo;': '"',
        '&rdquo;': '"',
        '&lsquo;': ''',
        '&rsquo;': ''',
        '&mdash;': '‚Äî',
        '&ndash;': '‚Äì',
        '&hellip;': '‚Ä¶',
        '&#8220;': '"',
        '&#8221;': '"',
        '&#8216;': ''',
        '&#8217;': ''',
        '&#8212;': '‚Äî',
        '&#8211;': '‚Äì',
        '&#8230;': '‚Ä¶',
        '&nbsp;': ' '
    }
    
    for entity, replacement in replacements.items():
        text = text.replace(entity, replacement)
    
    return text

def remove_emoji_from_category(category_name: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    clean_name = re.sub(r'[^\w\s]', '', category_name).strip()
    return clean_name

def validate_url(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def truncate_text(text: str, max_length: int = 4000) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è Telegram"""
    if len(text) <= max_length:
        return text
    
    # –û–±—Ä–µ–∑–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ
    truncated = text[:max_length - 3]
    
    # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
    last_sentence = truncated.rfind('.')
    if last_sentence > max_length * 0.8:  # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
        truncated = truncated[:last_sentence + 1]
    
    return truncated + '...'

def escape_markdown(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è Markdown"""
    escape_chars = ['_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!']
    for char in escape_chars:
        text = text.replace(char, f'\\{char}')
    return text

def format_processed_news(processed_news: Dict, source: str, image_url: Optional[str] = None) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram"""
    try:
        title = clean_html_entities(processed_news.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'))
        summary = clean_html_entities(processed_news.get('summary', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è'))
        hashtags = processed_news.get('hashtags', [])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if len(title) > 200:
            title = title[:197] + '...'
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è
        summary = truncate_text(summary, 3000)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        news_text = f"<b>{title}</b>\n\n"
        news_text += f"{summary}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        if source and source != '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫':
            news_text += f"üì∞ <i>{source}</i>\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à—Ç–µ–≥–∏
        if hashtags:
            hashtags_text = ' '.join(hashtags[:5])  # –ú–∞–∫—Å–∏–º—É–º 5 —Ö—ç—à—Ç–µ–≥–æ–≤
            news_text += f"\n{hashtags_text}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –¥–ª—è Telegram (–ª–∏–º–∏—Ç 4096 —Å–∏–º–≤–æ–ª–æ–≤)
        news_text = truncate_text(news_text, 4000)
        
        return news_text
        
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {str(e)}"

def clean_filename(filename: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    if len(filename) > 100:
        filename = filename[:100]
    return filename

def format_news_for_storage(news_item: Dict) -> Dict:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–µ—à–µ"""
    return {
        'id': news_item.get('id', ''),
        'title': clean_html_entities(news_item.get('title', '')),
        'summary': clean_html_entities(news_item.get('summary', '')),
        'link': news_item.get('link', ''),
        'published': news_item.get('published', ''),
        'source_title': clean_html_entities(news_item.get('source_title', '')),
        'image_url': news_item.get('image_url'),
        'processed_at': news_item.get('processed_at', ''),
        'category': news_item.get('category', ''),
        'processed_news': news_item.get('processed_news', {}),
        'language': news_item.get('language', 'unknown')
    }
